
# 1과목
### 데이터베이스의 일반적인 특징
- 통합된 데이터: 중복 X
- 저장된 데이터: IT 저장매체에 저장
- 공용 데이터: 여러 명이 공동 이용
- 변화하는 데이터: 최신 상태를 유지

### 가치 패러다임의 변화
Digitalization -> Connection -> Agency

# 2과목
## 분석 기획
### 분석 대상과 그 방법에 따른 4가지 분석 주제
p.71

### 목표 시점별 분석 기획
과제 단위 vs 마스터 플랜 단위 p.72


### 분석 기획시 고려사항
1. 가용 데이터
2. 적절한 활용 방안, 유스케이스 탐색
3. 장애요소 사전 계획 수립

### KDD 분석 방법론, CRISP-DM 분석 방법론
p.76

### 빅데이터 분석 방법론
p.78  
분석 기획 -> 데이터 준비 -> 데이터 분석 -> 시스템 구현 -> 평가 및 전개


## 분석 과제 발굴
하향식 vs 상향식  
하향식: 문제 탐색 -> 문제 정의 -> 해결방안 탐색 -> 타당성 검토

## 분석 프로젝트 관리 방안

### 능력 성숙도 통합 모델 CMMI
5단계로 구성, p.104

## 마스터플랜 수립

### 우선순위 고려 요소
- 전략적 중요도
- 비즈니스 성과/ROI
- 실행 용이성

### 적용 범위/방식 고려 요소
- 업무 내재화 적용 수준
- 분석 데이터 적용 수준
- 기술 적용 수준

### ROI를 활용한 우선순위 평가
시급성 - value  
난이도 - volume, variety, valocity

### 포트폴리오 사분면 우선순위 평가
p.110

## 데이터 거버넌스 체계 수립

### 분석 수준 진단
p.117

### 데이터 분석 조직 유형
기능 구조, 집중, 분산



<br>
<br>

# 3과목
## 데이터 탐색

### EDA

### 결측값
    R 기준으로, NA로 표현. 경우에 따라 null, 공백, -1 등 다양하게 표현될 수도 있다.  
    
    결측값은 삭제하는 것이 일반적. (단, 설문에서 결측값이 많다는 것은 해당 질문에 민감함을 나타내는 측도로 사용될 수도 있다.)

- 단순 대치법
- 평균 대치법
    - 비조건부 평균 대치법
    - 조건부 평균 대치법
- 단순 확률 대치법: 평균 대치법에서 추정량 표준 오차의 과소 추정 문제를 보완하고자 고안된 방법
    - K-Nearest Neighbor 방법
- 다중 대치법: 여러 대치를 통해 만드는 방법. 대치 - 분석 - 결합 3단계로 구성.

### 이상값
    이상값도 제거하는 것이 일반적이지만, 의미를 갖는 경우도 있으므로 목적이나 종류에 따라 사용자의 적절한 판단이 필요.

- ESD (Extreme Studentized Deviation): 평균에서 표준편차 3을 넘어가면 이상값.
- 사분위수: box plot. Q3 + 1.5 IQR과 Q1 - 1.5 IQR 을 넘어가면 이상값.

## 통계와 표본조사

### 표본추출 방법
- 단순 랜덤 추출법
- 계통 추출법: 일정한 간격으로 추출.
- 군집(Cluster / 집락) 추출법: 군집 간은 서로 동질적, 군집 내의 데이터는 이질적
- 층화 추출법: 층 강은 서로 이질적, 층 내의 데이터는 동질적.
    - 층별로 추출할 데이터의 비율을 정하는 방법에 따라, 비례 층화 추출, 불비례 층화 추출이 있다.

### 척도
- 질적 척도
    - 명목 척도
    - 순서 척도
- 양적 척도
    - 등간 척도 (구간 척도)
    - 비율 척도

### 기술통계와 추리통계

기술통계
- 표본 자체의 속성이나 특징을 파악하는데 중점.
- 자료를 요약, 조직화, 단순화하는 게 목적.

추리통계 (추론통계)
- 표본에서 얻은 통계치를 바탕으로 오차를 고려하면서, 모수를 확률적으로 추정하는 통계기법
- 모집단의 특성을 추정하기 위해, 가설을 검증하거나 확률적인 가능성을 파악함.


## 확률과 확률분포

### 확률
확률  
조건부 확률

독립사건  
배반사건

확률변수  
확률분포


## 이산 확률 분포

### 베르누이 분포

### 이항 분포

### 기하 분포

### 다항 분포

### 포아송 분포

### 이산 확률 변수

<br>

## 연속 확률 분포

### 균일 분포

### 정규 분포

### 표준 정규 분포

### t-분포

### 카이제곱 분포

### F 분포

### 지수 분포

### 연속 확률 변수


## 통계 개념

기댓값  
분산  
표준편차  
첨도  
왜도  
공분산  
상관계수


## 추정과 가설검정

### 추정

#### 모수 추정

#### 점추정

#### 구간추정

<br>

### 가설검정

#### 귀무가설
#### 대립가설
#### 제 1종 오류
#### 제 2종 오류
#### 검정통계량
#### 기각역, 유의수준, 유의 확률(=p-value)
p-value가 유의수준보다 작으면, 귀무가설 기각.

<br>

### 모수 검정 vs 비모수 검정

#### 모수 검정
- 표본이 정규성을 갖는다는 모수적 특성을 이용하는 통계 방법  

#### 비모수 검정
- 정규성 검정에서 정규분포를 따르지 않는다고 증명되거나, 표본 군집당 10명 미만의 소규모 실험이라 정규분포임을 가정할 수 없는 경우에 사용.
- 모수의 분포에 대해 어떠한 가정(정규분포)도 하지 않는 검정.

| 모수 검정 | 비모수 검정  |
|------------------------------------------------------------------------|-----------------------------------------------------------------|
| 등간척도, 비율척도 | 명목척도, 서열척도 |
| 평균  | 중앙값  |
| 피어슨 상관계수 | 스피어만 순위상관계수 |
| one sample t-test, two samplet t-test,<br>paired t-test, one way anova | 부호검정, Wilcoxon 부호 순위 검정, <br>Mann-Whitney 검정, Kruskal Wallis 검정 |

<br>
<br>

## 기초 통계

### t-검정

#### 단일 표본 t-검정 (One sample t-test)
    하나의 모집단의 평균값을 특정값과 비교하는 경우 사용하는 통계적 분석 방법.

#### 독립 표본 t-검정 (Independent sample t-test) (이 표본 t-검정)
    서로 독립적인 두 개의 집단에 대하여, 모평균이 같은 값을 갖는지 통계적으로 검정하는 방법

    등분산성을 만족해야 한다. 따라서 F 검정을 먼저 해야 함.

#### 대응 표본 t-검정 (Paired t-test)
    동일한 대상에 대해 두 가지 관측치가 있는 경우, 둘의 차이가 있는지 검정

    주로 실험 전후의 효과 비교에 사용


### 분산분석 ANOVA
세 개 이상의 모집단이 있을 경우, 이들의 평균을 비교하는 검정 방법.  

- 귀무가설은 항상 "$H_0$: 모든집단 간 평균은 같다."

- 귀무가설을 기각할 경우 어느 집단 간 평균이 같은지, 혹은 얼마나 다른지 알수 없다는 단점이 있다. -> 사후 검정 방법으로 Scheffe, Tuckey, Duncan, Fisher's LSD, Dunnett, Bonferroni 등의 방법을 사용하면 된다.
- 독립변수는 범주형, 종속변수는 연속형 이어야 한다.
- "집단 간 분산 / 집단 내 분산" 인 F-value가 사용됨.
- 분산 분석을 위한 세 가지 가정사항  
>1. 정규성: 각 집단의 표본들은 정규분포를 따라야 한다.
>2. 등분산성: 각 집단은 동일한 분산을 가져야 한다.
>3. 독립성: 각 집단은 서로에게 영향을 주지 않는다.

#### 분산분석표
p.220

#### 일원분산분석 (One-way Anova)
    독립 변수가 하나 일 때.

#### 이원분산분석 (Two-way Anova)
    독립 변수가 두 개 이상일 때.  
    독립변수 간 교호작용이 있다고 판단될 때는 '반복이 있는 실험'을 하고, 교호작용이 없다고 판단될 때는, 즉 두 독립변수가 독립인 경우에는 '반복이 없는 실험'을 한다.

#### 다변량분산분석 = 다원분산분석 (Manova)
    종속 변수가 두 개 이상일 때.


### 교차 분석

    범주형 자료 간의 관계를 알아보고자 할 때 사용하는 분석방법

- 카이제곱 검정통계량을 사용
- 적합도 검정, 독립성 검정, 동질성 검정에 사용
- 교차 분석표
    - 두 범주형 변수를 교차하여 데이터의 빈도를 표 형태로 나타낸 것

#### 적합도 분석
    실험결과 얻어진 관측값이 예상값과 일치하는지 여부를 검정
- $H_0$ : 실제 분포와 예측 분포간에는 차이가 없다.
- $a$ : 0.05
- $\chi _{a, df}$　 (자유도 df = 범주수 -1)


#### 독립성 분석
    모집단이 두 개의 변수에 의해 범주화됐을 때, 그 두 변수들 사이의 관계가 독립적인지를 검정
- 카이제곱 검정에 의한 독립성 검정 결과는, 두범주형 변수 간에 관계가 있는지 없는지만을 나타낼 뿐이며, 강도를 말해주지 않는다. 따라서 상관관계의 강도를 알기 위해서는 상관분석을 해야 한다.
#### 동질성 분석
    관측값들이 정해진 범주 내에서 서로 비슷하게 나타나는지를 검정.
    즉, 두 집단의 분포가 동일한 모집단에서 추출된 것인지를 검정.
    

### 상관 분석
    두 변수 간의 선형적 관계가 존재하는지 알아보는 분석 방법.
    상관계수를 활용.
- 상관간계가 있단느 것이 반드시 인과관계가 있다는 말은 아니다. 상간관계는 존재하지만 인과관계는 없을 수도 있다.
- 상관분석의 귀무가설은 "$H_0$ : $\gamma_{xy}$ = 0 (두 변수는 상관관계가 없다.)"

#### 산점도 행렬

#### 피어슨 상관분석 (선형적 상관관계)
    모수적 방법의 하나로, 두 변수가 모두 정규분포를 따른다는 가정이 필요.

#### 스피어만 상관분석 (비선형적 상관관계)
    비모수적 방법으로, 관측값의 순위에 대해 상관계수를 계산하는 방법.
    두 변수들이 서열척도일 때 사용.

<br>

## 회귀 분석

    하나 이상의 독립변수들이 종속변수에 얼마나 영향을 미치는지 추정
- 독립변수와 종속변수 간에 인과관계가 있다.
- 독립변수가 하나면 단순선형회귀분석, 독립변수가 여러개면 다중선형회귀분석
- 보통 연속형 변수일 때 사용. 범주형 변수는 파생변수로 변환해야 함. 만약 종속변수가 범주형이라면 로지스틱 회귀분석.

### 회귀  분석의 가정
- 선형성
- 독립성
- 등분산성
- 정규성
p.233

### 단순 선형 회귀 분석
- 최소제곱법으로 오차가 작아지는 회귀추세선을 찾는다.

**회귀분석 모형의 적합성**  
p.235

### 다중 선형 회귀 분석

**다중공선성**  
- 진단
    - 결정계수 값이 커서 회귀식의 설명력은 높지만, 각 독립변수의 p-value가 커서 개별 인자가 유의하지 않을 경우, 의심 가능.
    - 독립변수 간 상관계수를 구하거나, VIF (분산팽창요인)이 10을 넘기면 다중공선성이 있음.
    - VIF = $\displaystyle\frac{1}{1-R^2}$
- 해결
    - 변수를 제거
    - PCA(주성분분석)로 차원 축소
    - SVD(특잇값 분해)로 차원 축소
    - LDA(선형판별분석)로 차원 축소
    - t-SNE(t-분포 확률적 임베딩)로 차원 축소

### 최적 회귀 방정식
    종속변수 y를 가장 잘 설명하는 독립변수 후보를 찾는 것.
- 부분집합법
    - 모든 가능한 모델을 고려하여 가장 좋은 모델을 성정하는 방법
    - 변수의 개수가 적은 경우, 높은 설명력을 가진 결과를 도출하는데 효과적. 변수가 많으면 검증해야하는 회귀 분석도 많아지는게 단점.
    - 임베디드 기법이라고도 함. 라쏘, 릿지, 엘라스틱넷 등의 다양한 방법을 사용
- 단계적 변수선택법

<br>

**벌점화(페널티 penalty) 방식　AIC, BIC**
- 회귀 모형은 변수의 수가 증가할수록 편향(bias)은 작아지고 분산은 커지는 경향이 있다.

> AIC (Akaike Information Criteria)  
> - MSE에 변수 수만큼 페널티를 주는 지표.  
> - 일반적으로 회귀분석에서 Model Selection할 때 많이 씀  
>
> BIC
> - AIC의 단점인, 표본의 수(n)가 커질 때 부정확하다는 단점을 보완한 지표.
> - BIC는 변수의 개수가 많을수록 AIC보다 더 큰 페널티를 주기 때문에, 변수의 개수가 적은 모형이 우선이라면 BIC를 참고하는 것을 권장.
>
> 멜로우 Cp (Mallow's Cp)
> - Cp값은 최소자승법으로 사용하여 추정된 회귀모형의 적합성을 평가하는데 사용.
> - Cp값은 수정된 결정계수 및 AIC와 밀접한 관련이 있다.
> - Cp값이 p값보다 크면 나쁜 모델, Cp값이 p값보다 작으면 좋은 모델.

<br>

**단계적 변수 선택법**
> 전진선택법  
> - 상관계수 절댓값이 큰 변수에 대해 부분 F 검정을 실시한다. 그리고 설명력이 가장 높은 설명변수(p-value가 가장 작은 변수)부터 선택해나간다.  
>
> 후진제거법  
> 단계별 방법  

<br>

### 고급 회귀 분석

#### 정규화 선형회귀
**과적합 & 과소적합**

**정규화 선형회귀**
- 라쏘
- 릿지
- 엘라스틱넷

#### 일반화 선형회귀
**로지스틱 회귀**  
**포아송 회귀**

#### 더빈 왓슨 검정

<br>
<br>

## 다변량 분석

### 다차원 척도법
    객체 간의 근접성을 시각화하는 통계기법
- 유클리드 거리행렬을 사용.
- stress 값 사용. 0 ~ 1로, 0은 완벽, 0.05이내면 적합도가 좋고, 0.15 이상은 적합도가 안 좋은 것.

### PCA　주성분 분석
    서로 상관성이 높은 변수들을 선형 결합하여 새로운 변수를 만들고 차원을 축소

- 손실이 가장 작은 축을 찾아야 함. 즉, 자료의 분산이 가장 큰 축을 찾아야 함.
- 고유값?

<br>
<br>

## 시계열 분석

**시계열 자료의 자기상관성**

    서로 이웃하는 자료들끼리 일종의 상관관계를 가진다.  
    잔차항의 크기가 이웃하는 다른 잔차항의 크기와 서로 일정한 관련이 있다.
- 정상성 시계열 자료
    - 평균이 일정, 분산이 시점에 의존하지 않아야 함.
- 비정상성 시계열 자료
    - 정상성을 만족하지 못 하는 시계열
    - 대부분의 시계열 데이터가 여기에 속함.

**시계열 자료의 정상성 조건**  

1. 일정한 평균
    - 모든 시점에 대해 평균이 일정해야 함.
    - 일정하지 않다면, 차분으로 정상화 가능.
    - 계절적인 주기가 있는 경우, 여러 시점 전의 자료값을 빼는 계절차분을 사용.
2. 일정한 분산
    - 모든 시점에서 분산이 일정해야 함.
    - 일정하지 않다면, 변환으로 정상화 가능.
    - 지수 변환, 로그 변환을 취해서 시간에 따라 변하는 분산의 크기를 안정시킴.
3. 시차에만 의존하는 공분산
    - 공분산은 시차에만 의존하고, 시점에는 의존하지 않는다.

**자기상관계수**
- 자기상관계수
    - "시간의 흐름"에 따른 자기상관관계
    - 두 시계열 확률변수 간의 상관관계
- 부분자기상관계수
    - 두 시계열 확률변수 간에 다른 시점의 확률변수 영향력은 통제하고 상관관계만 보여줌.

**시계열 분석 기법**
- 이동평균법
    - 일정 기간별로 자료를 묶어 평균을 구하는 방법
    - 장기적인 추세를 파악하는데 효율적. 하지만 모든 시점에서 동일한 가중치를 주기 때문에, 최근의 자료가 더 많은 정보를 갖고 있다는 일반적인 견해에 어긋남.
- 지수평활법
    - 최근 데이터에 더 큰 가중치를 부여해서 평균을 구함.
    - 자료의 수가 많고 자료가 안정된 패턴을 보이는 경우, 예측 품질이 높음.
    - 불규칙변동의 영향을 제거할 수 있으므로 중장기 예측에 사용.

### 시계열 모형

**자기 회귀 (AR, Autoregressive)**

    t라는 시점에서의 값 yₜ는 이전 시점들 n개에 의해 설명될 수 있다.  
    이전 시점들의 선형 결합.
- 부분자기상관함수를 사용.

**이동 평균 (MA, Moving Average)**

    이전 시점의 백색잡음들의 선형 결합.
- 백색잡음들의 선형 결합으로 이루어져 있기 때문에, 항상 정상성을 만족.
- 자기상관함수를 사용.

**자기회귀누적이동평균모형 (ARIMA, )**

    비정상 시계열을 다룰 수 있는 모형.
- 현실에 존재하는 대부분의 시계열 자료를 설명할 수 있음.
- 비정상 시계열이기 때문에, 차분이나 변환을 통해 정상화 가능.
- ARIMA(p, d, q)
    - p: AR 모형의 차수
    - d: 정상화하기 위한 필요 차분 횟수
    - q: MA 모형의 차수

**분해 시계열**

    분석 목적에 따라 특정 요인만 분리해 분석하거나 제거하는 작업을 함.
- 추세요인: 장기간 일정한 방향으로 상승 또는 하락.
- 계절요인: 주기가 일정하지 않은 변동. 추세선을 따라 상하로 반복 운동.
- 순환요인: 일정한 주기를 가지는 변동.
- 불규칙요인: 위 세 가지 요인으로 설명 못하는 오차에 해당하는 요인.


<br>
<br>

# 데이터 마이닝
    데이터 마이닝(Data Mining)은 방대한 데이터 속에서 숨겨진 규칙, 패턴 등을 찾아내어 예측하거나 의사결정에 활용하는 것이 목적

**데이터 마이닝의 종류**
| 지도 학습 | 비지도 학습 |
|---|---|
| 회귀 | 군집 |
| 분류 | 연관 |
|   | 차원축소 |

**데이터 마이닝의 프로세스**  

    목적 정의 -> 데이터 준비 -> 데이터 가공 -> 데이터 마이닝 기법 적용 -> 검증

### 데이터 분할
일반적으로, train : validation : test = 8 : 2 : 2

**홀드아웃**  
- validation 없이, train : test = 8 : 2
- 데이터 수가 적을 경우, 각 데이터셋이 전체 데이터를 대표하지 못할 가능성이 큼.

**K-Fold 교차검증 (K-Fold Cross Validation)**  
- k-1개의 train set, 1개의 test set으로 나눠서, k개의 모델을 만들고 종합하여 최종 모델을 구축.
- 데이터가 적으면 과적합 방지가 어려울 수도 있다.
- LOOCV: k가 n임.

**부트스트랩 (Bootstrap)**  
- 원본 데이터 크기와 동일한 크기로, 표본을 랜덤하게 복원추출함.
- 데이터셋의 분포가 고르지 않아 오버샘플링 혹은 언더샘플링과 같은 문제가 있을 때 사용될 수 있다?
- 부트스트랩으로 한번도 선정되지 않을 확률은 36.8%로, 이 데이터를 test로 사용

**계층별 k-겹 교차 검증 (Stratified K-Fold Cross Validation)**  
- 불균형 데이터일 때, 각 fold가 가지는 레이블의 분포가 유사하도록 추출함.

**오버샘플링 & 언더샘플링**

<br>
<br>

## 분류 분석

### 로지스틱 회귀분석
    종속변수가 범주형. 독립변수의 선형 결합을 이용해 범주에 포함될 확률값을 리턴.
- 독립변수는 연속형이어야 함. 만약, 독립변수가 범주형이라면, 더미변수로 변환해서 사용 가능.
- 이진 분류라면 로지스틱 회귀분석, 3개 이상으로 분류하면 다중 로지스틱 회귀분석.

#### 오즈(Odds) & 로짓(Logit) 변환 & 시그모이드(Sigmoid)
>**오즈(Odds)**  
> - 성공할 확률이 실패할 확률의 몇 배인지를 나타내는 값
> - Odds = $\displaystyle\frac{P}{1-P}$
>  
>**로짓(Logit)**
> - 오즈는 두 가지 한계가 있다. 음수를 가질 수 없고, 확률값과 오즈의 그래프는 비대칭성을 띈다.
> - 이 한계를 극복하기 위해 오즈에 로그값을 취한 것이 로짓.
> - -∞ < y=log(Odds) < ∞ , 0 < x=P < 1 , 성공확률 0.5기준으로 대칭
>
>**시그모이드(Sigmoid)**
> - 로짓 함수의 역함수
> - 로지스틱 회귀분석과 신경망에서 활성화 함수로 사용.
> - 어떤 사건이 발생할 추정 확률을 의미.

### 의사결정나무
    분리규칙을 찾아내고, 소집단으로 분류함
- 하위 노드로 갈수록 **집단 내에서는 동질성이 커지고, 집단 간에는 이질성이 커진다.**
- 종속변수가 범주형이면 분류 트리, 종속변수가 연속형이면 회귀트리.

#### 의사결정나무의 활용
세분화, 분류, 예측, 차원축소 및 변수 선택, 교호 작용

#### 의사결정나무 특징
| 장점 | 단점 |
|----|----|
| 모델이 직관적이고 해석이 용이 | 독립변수들 사이의 중요도를 판단하기 쉽지 않음 |
| 데이터 정규화 및 단위변환이 필요 없다. | 분류 경계선 근처의 자료는 오차가 큼 |
| 데이터의 선형성, 정규성등의 가정이 불필요. | 과적합 발생 가능성이 높음 |
| 이산형, 연속형 모두 가능 |    |
| 이상값에 민감하지 않음 |    |

#### 의사결정나무 분석 과정
> 1. 성장
>    - 적절한 분리 기준과 정지규측을 설정해 의사결정나무를 성장시킴
>    - 불순도 감소량이 가장 커지도록 분할.
>       - 종속변수가 이산형 (분류 트리): 카이제곱 검정, 지니 지수, 엔트로피 지수
>       - 종속변수가 연속형 (회귀 트리): 분산분석에서의 F통계량, 분산의 감소량
>    - 정지규칙 4가지
>       - 뿌리마디로부터 일정 깊이에 도달했을 때
>       - 마디에 속하는 자료가 일정 수 이하일 때
>       - 불순도의 감소량이 아주 작아 분리에 의미가 없을 때
>       - 모든 자료들이 하나의 그룹에 속할 때
> 2. 가지치기
> 3. 타당성 평가
> 4. 해석 및 예측

#### 지니 지수 계산
p.304

### 앙상블
- 종속변수의 형태에 따라, 분류분석, 회귀분석 둘다 가능.
- 범주형이면 다수결 방식, 수치형이면 평균을 사용.

#### 배깅 (Bagging, Bootstrap Aggregating)
    여러 부트스트랩으로 각 모델(분류기, 의사결정나무)을 만들고 집계함
- 각각의 부트스트랩을 구성할 때 복원추출을 하기 때문에, 부트스트랩은 알 수 없던 모집단의 특성을 더 잘 반영할 수 있다. 배깅은 모집단의 특성이 잘 반영되는 분산이 작고 좋은 예측력을 보여줌.

#### 부스팅 (Boosting)
    약한 모델들을 결합해나감을써 점차적으로 강한 분류기를 만듦.
- 이전 모델을 구축한 뒤, 다음 모델을 구축할 때, 이전 분류기에 의해 잘못 분류된 데이터에 더 큰 가중치를 주어 부트스트랩을 구성함.
- 배깅은 모델(분류기)이 서로 독립적이지만, 부스팅은 독립적이지 않다.
- 잘못 분류된 데이터에 가중치를 주기 때문에 훈련 오차를 빠르게 줄일 수 있다. 예측 성능도 배깅보다 뛰어나다고 할 수 있다.
- AdaBoosting, Gradient Boost, XGBoost, Light GBM 등

#### 랜덤 포레스트 (Random Forest)
    각 마디에서 비복원추출을 한 번더 한 후, 추출된 표본을 대상으로 최적의 분할을 함.
- 배깅에서는 트리의 마디에서 불순도가 가장 작아지는 최적의 분할을 한다. 하지만 랜덤 포레스트에서는 마디에서 표본추출을 한 번 더 하고, 추출된 표본을 대상으로 최적의 분할을 한다.
- 큰 분산을 갖는다는 의사결정나무의 단점을 보완. 분산을 감소.
- 따라서 모든 분류기들이 높은 비상관성을 갖기 때문에, 일반화 능력 향상.
- 의사결정나무의 특징을 이어받아 이상값에 민감하지 않음.

### 인공신경망
| 장점 | 단점 |
|----|----|
| 잡음에 민감하지 않음 | 모델이 복잡할 경우, 학습이 오래 걸림 |
| 비선형적인 문제에도 유용 | 초기 가중치에 따라 전역해가 아닌 지역해로 수렵할 수도 있다 |
| 패턴인식, 분류, 예측 등에 효과적 | 추정한 가중치의 신뢰도가 낮다 |
| 다양하고 많은 데이터에 효과적 | 결과 해석이 어려움 |
|    | 은닉층 수와 은닉 노드 수를 결정하기 어려움 |

#### 활성함수 (Activation Function)
    노드의 출력을 바로 다음 노드로 전달하지 않고, 비선형 함수를 통과시킨다.  
    활성함수에 따라 출력값이 달라지므로 적절한 활성함수를 선택애햐 함.
- Step
- Sign
- Sigmoid
- tanh
- Relu
- Softmax

역전파 알고리즘  
단층 퍼셉트론  
다층 퍼셉트론  
RNN, CNN, LSTM, YOLO, GAN

### 나이브베이즈 분류
#### 베이즈 이론 (Bayes Theorem) (베이지안 확률)
- 통계학의 확률은 빈도 확률과 베이지안 확률로 나뉨
    - 빈도확률은 객관적으로 확률을 해석하고, 베이지안 확률은 주관적으로 확률을 해석.
- 빈도확률: 사건이 발생한 횟수의 장기적인 비율을 의미.
- 베이지안 확률: 사전확률과 우도확률을 통해 사후확률을 추정하는 정리.
    - 현재 관측된 데이터의 빈도만으로 분석하는 것이 아니라, 분석자의 사전지식까지 포함해 분석하는 방법

#### 나이브 베이즈
- 베이즈 정리를 기반으로 한 지도학습 모델
- 스팸 메일 필터링, 텍스트 분류 등

### k-NN (k-Nearest Neighbor)
    정답 라벨이 없는 새로운 데이터를 입력 받았을 때, 가장 가까운 데이터의 라벨을 확인하여 결정하는 것
- 지도학습인 분류에 속하지만, 군집의 특성도 가지고 있어서, 경우에따라 준지도학습으로 분류하기도 함.
- 오직 지역적으로 근사하고, 모든 계산이 분류될 때까지 연기되는 인스턴스 기반 학습이다. 그래서 게으른 학습이라고도 함.

### SVM 서포트 벡터 머신
    초평면(hyper-plane)을 이용하여 카테고리를 나누어 비확률적 이진 선형모델을 만듦
- 가장 높은 마진을 가져가는 방향으로 분류.

### 분류 모형 성능 평가
#### 혼동행렬 (Confusion Matrix) 오분류표
|   | Predicted Positive | Predicted Negative |
|---|---|---|
| Actual Positive | True Positive (TP) | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TF) |

|   |   |   |
|---|---|---|
| 정확도<br>= 정분류율 (Accuracy) |   |   |
| 오차 비율<br>= 오분류율 (Error Rate) |   |   |
| 참긍정률 (TP Rate)<br>= 재현률 (Recall)<br>= 민감도 (Sensitivity) |   | 실제 긍정 중 긍정으로 올바르게 예측한 비율 |
| 특이도 (Specificity) |   | 실제 부정 중 부정으로 올바르게 예측한 비율 |
| 거짓 긍정률 (FP Rate) |   | 실제 부정 중 긍정으로 잘못 예측한 비율 |
| 정밀도 (Precision) |   | 긍정으로 예측한 비율 중 실제로 긍정인 비율 |
| F1 지표 (F1 Score) |   | 정밀도와 재현율을 조화평균 |

#### ROC 커브 (Receiver Operating  Characteristic Curve)
- x축은 FPR (1-특이도), y축은 TPR (재현율, 민감도)
- 이진 분류 모형의 성능을 평가하는데 사용
- ROC 커브의 아래 면적은 AUROC로, 1에 가까울수록 모델 성능이 우수하고, 0.5에 가까울수록 무작위로 예측하는 나쁜 모델이 됨.

#### 이익도표 (Lift Chart)

#### 향상도 곡선 (Lift Curve)

<br>
<br>

## 군집분석
    유사성을 측정하고 유사한 자료들끼리 군집으로 묶고,
    다변량 분석(상관분석, 회귀분석, 주성분분석 등)을 활용해 각 군집의 특징을 파악하는 것

### 거리 측도
#### 연속형의 거리 측도
>**유클리디안 거리(Euclidean)**  
>- 변수들의 산포정도를 고려하지 않음.  
>
>**맨하튼 거리(Manhattan)**  
>**체비셰프 거리(Chebychev)**  
>- 변수 간 거리 중 최댓값을 데이터 간의 거리로 정의  
>
>**표준화 거리(Standardized)**
>- 유클리디안 거리에서 변수 간 단위의 차이로 발생하는 문제를 표준편차로 나눠서 해결
>
>**마할라노비스 거리**
>- 표준화 거리가 고려하지 못한 변수 간 상관성까지 고려한 거리
>
>**민코프스키 거리**
>- 유클리디안 거리와 맨하튼 거리를 한번에 표현한 거리. m=1일 때 맨하튼, m=2일 때 유클리디안

#### 범주형의 거리 측도
>**단순 일치 계수**
>- m/p , p는 총 변수 개수, m은 일치하는 변수의 수
>
>**자카드 지수**
>- 두 집합의 유사도를 측정. 두 집합이 같으면 1, 완전 다르면 0
>
>**자카드 거리**
>- 1 - 자카드 지수
>
>**코사인 유사도**
>- 문서(텍스트)의 유사도를 측정하기 위한 지표. 크기가 아닌 방향성을 측정.
>- 방향이 일치하면 1, 완전 다른 방향이면 -1
>
>**코사인 거리**
>- 1 - 코사인 유사도
>
>**순위 상관 계수**
>- 순서 척도인 두 데이터 사이의 거리를 측정. 스피어만 상관계수.


### 계층적 군집분석
    병합적 방법과 분할적 방법이 있다.
- 병합적 방법이 대표적. 몇 개의 군집으로 나눌 것인지를 사전에 정의할 필요가 없고, 분석 결과를 바탕으로 분석가가 판단하여 설명 가능한 수준으로 군집화할 수 있다.
- 두 데이터(혹은 군집)를 하나의 군집으로 묶었다면, 새로운 군집과 기존의 데이터 사이의 거리를 다시 측정해야 한다. 이때 여러 방법이 존재한다.
- 범주형 데이터에서도 거리 측정이 가능하므로 적용 가능?

#### 군집 간의 거리
>**단일연결법 = 최단 연결법**  
>**완전연결법 = 최장 연결법**  
>**평균연결법**  
>**중심연결법**  
>**와드연결법**
>- 군집내 편차 제곱합이 최소가 되도록 연결


### 비계층적 군집분석
    구하고자 하는 군집의 수를 사전에 정의
- 데이터 간 거리 행렬을 사용하여 분석을 수행하지 않는다.

#### K-means 군집
- 군집 수 k를 설정하고, k개의 seed를 임의로 설정하거나 데이터 중 직접 선택한다.
- "집단 내 제곱합 그래프"로 k값 결정에 도움을 받을 수 있다.

| 장점 | 단점 |
|----|----|
| 비교적 단순하고 빠르다 | 군집 수 k를 설정하기 어려움 |
| 다양한 데이터에서 사용 가능 | 결과의 해석이 어려울 수 있음 |
|    | 연속형 변수이어야 함 |
|    | 안정된 군집은 보장하나 최적의 보장은 없음 |
|    | 이상값에 민감하게 반응(중앙값으로 해결O) |

#### K-medoids

#### DBSCAN (Density Based Spatial Clustering of Applications with Noise)
- 거리 기반이 아닌, 밀도 기반 군집분석.
- k-means와 달리 군집의 형태에 구애받지 않기 때문에, 기하학적이고 노이즈가 포함된 데이터셋에서도 효과적으로 군집할 수 있고, 군집 수를 설정할 필요가 없다.

### 혼합 분포 군집
    모형기반 군집 방법.  
    관측된 데이터들은 여러 개의 확률분포(흔히 정규분포)로부터 추출됐다는 가정하에
    같은 확률분포에서 추출된 데이터들끼리 군집화하는 분석 기법

#### EM 알고리즘

### SOM 자기조직화지도
    인공신경망 기반 차원 축소와 군집화를 동시에 하는 알고리즘.
- 다차원 데이터를 축소해서 저차원의 지도를 생성하고, 이를 시각화 함.
- 은닉층 없이, j개의 데이터를 입력받는 입력층, n개의 노드로 표현하는 경쟁층으로 구성되며, 완전연결(Fully connected)됨.

| 장점 | 단점 |
|---|---|
| 역전파를 사용하지 않는 순전파 방식이므로 속도가 빠름 | 초기 학습률, 초기 가중치에 영향을 많이 받음 |
| 저차원의 지도로 표현되므로 시각적 이해가 쉬움 | 경쟁층의 이상적인 노드 수를 결정하기 어려움 |
| 패턴 발견 및 이미지 분석에서 우수 |   |
| 입력 데이터의 속성을 그대로 보존 |   |

### 군집분석 모형의 평가

- 외부 평가
    - 자카드 계수 평가
    - 분류 모형 평가 방법을 응용
- 내부 평가
    - 단순 계산법
        - $\displaystyle\sqrt{\frac{n}{2}}$ (n = 전체 데이터 수)
    - 군집 간의 거리를 계산하여 평가
    - **실루엣 계수**
        - 하나의 데이터와 나머지 모든 데이터와의 거리를 활용해 지금 데이터 속한 군집 안에 데이터들이 잘 속해 있는지 평가.
        - -1 ~ 1을 가짐. 1에 가까울 수록 군집이 잘된 것.
    - 엘보 메소드

## 연관 분석
    장바구니 분석
- 탐색적 기법의 일종으로, 조건 반응에 의해 표현되므로 결과를 쉽게 이해할 수 있다.
- 품목 수가 증가하면 계산이 기하급수로 증가함 -> 품목을 하나의 범주로 일반화해서 해결 -> 단, 너무 세분화된 품목으로 연관 규칙을 찾으면 의미없는 분석이 될 수도 있다.

| 장점 | 단점 |
|---|---|
| 결과가 단순하고 분명함 | 품목 세분화에 어려움이 있다 |
| 계산이 간단 | 품목 수의 증가는 기하급수적인 계산량 증가를 초래 |
| 목적변수가 없으므로 데이터 탐색을 위해 사용 가능 | 거래가 발생하지 않은 품목은 분석 불가 |


### 연관 분석의 측도
    연관분석을 하면 무수히 많은 연관 규칙이 생기는데,
    모든 연관 규칙이 유용하지는 않으므로 측도를 통해 유의미한지 확인해야 함.

#### 지지도
- 전체 거래 중에서 A와 B라는 두 품목을 다 구매한 비율.
- 지지도가 높다는 것은, 두 품목이 같이 잘 팔린다는 뜻
- $P(A \cap B)$
#### 신뢰도
- 어느 한 품목을 구매할 때, 다른 품목도 구매될 확률.
- A를 구매했을 때 B를 구매할 확률(A→B)과 B를 구매했을 때 A를 구매할 확률(B→A)은 다름.
- (A→B) = P(B|A) = $\displaystyle\frac{P(A \cap B)}{P(A)}$
#### 향상도
- A없이 B를 구매할 확률 대비 A와 B 둘다 구매할 확률
- 향상도(A→B)와 향상도(B→A)는 같다.
- $\displaystyle\frac{P(A \cap B)}{P(A)\cdot P(B)}$
- 향상도 < 1 음의 상관관계 , = 1 관계없음 , > 1 양의 상관관계

### apriori 알고리즘
    지지도를 사용해 빈발 아이템 집합을 판별하고, 이로써 계산 복잡도를 감소
- 품목 증가에 따른 계산량 증가와 낮은 지지도인 품목은 의미 없는 결과를 도출할 수 있는 문제를 해결하고자 "최소 지지도"를 도입했으나, 여전히 많은 계산량이 필요

### FP-Growth 알고리즘
    지지도가 낮은 품목부터 높은 품목으로 올라가면서, 빈도수가 높은 아이템 집합을 생성하는 상향식 알고리즘
- apriori 보다 속도가 빠르고, 비용이 저렴.

### 순차 패턴
연관분석에 시간 개념이 추가


































